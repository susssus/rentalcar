name: Daily price scrape

on:
  schedule:
    # Noon UTC daily
    - cron: "0 12 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps chromium || true

      - name: Run scraper and ingest
        id: scrape
        timeout-minutes: 2
        env:
          INGEST_URL: ${{ secrets.INGEST_URL }}
          INGEST_SECRET: ${{ secrets.INGEST_SECRET }}
        run: |
          set -e
          echo "=== 1. Running scraper ==="
          # Logs go to stderr, JSON is last line of stdout; tee captures both so we show logs and extract JSON
          python scripts/run_and_output_json.py 2>&1 | tee /tmp/scraper.log
          OUT=$(tail -n1 /tmp/scraper.log)
          echo ""
          echo "Scraper log (above). JSON (first 800 chars):"
          echo "$OUT" | head -c 800
          echo ""

          NUM_OFFERS=$(echo "$OUT" | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('num_offers', 0))" 2>/dev/null || echo "0")
          echo "num_offers=$NUM_OFFERS" >> $GITHUB_OUTPUT
          echo "zero_offers=$([ "$NUM_OFFERS" = "0" ] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT

          if [ -z "$INGEST_URL" ] || [ -z "$INGEST_SECRET" ]; then
            echo "=== FAIL: INGEST_URL or INGEST_SECRET not set in repo Secrets. Add them in Settings → Secrets and variables → Actions. ==="
            exit 1
          fi

          # Accept any valid run JSON (even with null min_price) so we record that a run happened
          if ! echo "$OUT" | python3 -c "import sys,json; d=json.load(sys.stdin); exit(0 if d.get('pickup_date') else 1)" 2>/dev/null; then
            echo "=== FAIL: Scraper did not return valid JSON with pickup_date (maybe it crashed or timed out). Check the scraper output above. ==="
            exit 1
          fi

          BASE="${INGEST_URL%/}"
          INGEST="${BASE}/api/ingest"
          echo "=== 2. POSTing to $INGEST ==="
          HTTP=$(curl -sS -w "%{http_code}" -o /tmp/ingest_resp -X POST "$INGEST" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $INGEST_SECRET" \
            -d "$OUT")
          echo "Response ($HTTP):"
          cat /tmp/ingest_resp
          echo ""

          if [ "$HTTP" != "200" ]; then
            echo "=== FAIL: Ingest returned HTTP $HTTP. If 401: INGEST_SECRET in GitHub must match CRON_SECRET or INGEST_SECRET in Vercel. If 500: check Vercel logs / Redis. ==="
            exit 1
          fi
          echo "=== Done: data was stored. Refresh your dashboard. ==="

      - name: Dump HTML when 0 offers (debug artifact)
        if: steps.scrape.outputs.zero_offers == 'true'
        timeout-minutes: 2
        run: |
          echo "=== 0 offers: saving page HTML as artifact for debugging ==="
          python -c "
          import sys
          sys.path.insert(0, '.')
          from pathlib import Path
          from scraper import dump_page_html
          dump_page_html(Path('scrape-zero-offers.html'))
          "
          echo "Saved scrape-zero-offers.html"

      - name: Upload HTML artifact (when 0 offers)
        if: steps.scrape.outputs.zero_offers == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: scrape-zero-offers-html
          path: scrape-zero-offers.html
